{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Situation\n",
    "\n",
    "At the time of this experiment, Udacity courses currently have two options on the course overview page: \"start free trial\", and \"access course materials\". If the student clicks \"start free trial\", they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. If the student clicks \"access course materials\", they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.\n",
    "\n",
    "# Experiment\n",
    "\n",
    "In the experiment, Udacity tested a change where if the student clicked \"start free trial\", they were asked how much time they had available to devote to the course. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. [This screenshot](https://www.google.com/url?q=https://drive.google.com/a/knowlabs.com/file/d/0ByAfiG8HpNUMakVrS0s4cGN2TjQ/view?usp%3Dsharing&sa=D&ust=1566807927543000) shows what the experiment looks like.\n",
    "\n",
    "The unit of diversion is a cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some sensible defaults\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "$H_0$: This change won't set clearer expectations for students upfront, and not reduce the number of frustrated students who leave the free trial because they don't have enough time. It won't significantly reduce the number of students to continue past the free trial and eventually complete the course.\n",
    "\n",
    "$H_1$: This change will set clearer expectations for students upfront, thus reducing the number of frustrated students who leave the free trial because they don't have enough timeâ€”without significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis holds true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.\n",
    "\n",
    "## Metric Choice\n",
    "\n",
    "There are several possible metrics that could be used for the experiment.\n",
    "\n",
    "### Invariant Metrics\n",
    "\n",
    "Invariant metrics are chosen to investigate possible issues in the experiment setup and execution, i.e. detect bad measurements due to errors or unintended consequences.\n",
    "\n",
    "#### Selected:    \n",
    "\n",
    "| Metric Name               | Formula                                 | $d_{min}$ | Notation         |\n",
    "|---------------------------|-----------------------------------------|-----------|------------------|\n",
    "| Number of cookies         | # cookies on course overview page       | 3000      | $cookies_{uniq}$ |\n",
    "| Number of clicks          | # cookies clicked on button             | 240       | $clicks_{uniq}$  |\n",
    "| Click-Through-Probability | $\\frac{clicks_{uniq}}{cookies_{uniq}}$  | 1%        | $CTP$            |\n",
    "\n",
    "- **Number of cookies**: That is, number of unique cookies to view the course overview page. (dmin=3000)\n",
    "    - A good invariant as this is the unit of diversion, hence it is randomized by definition. This metric should not be significantly different from a p=0.5 value (for an equal split)\n",
    "- **Number of clicks**: That is, number of unique cookies to click the \"Start free trial\" button (which happens before the free trial screener is trigger). (dmin=240)\n",
    "    - Also a good invariant, even though it is a subset of cookies from above the experiment happens after the button is clicked and therefore the cookies should not be significantly different between groups.\n",
    "- **Click-through-probability**: That is, number of unique cookies to click the \"Start free trial\" button divided by number of unique cookies to view the course overview page. (dmin=0.01)\n",
    "    - The click-through-probability from course overview page to \"Start free trial\" click should be unaffected by the experiment, hence not significantly different between groups.\n",
    "\n",
    "#### Not Selected:    \n",
    "\n",
    "| Metric Name               | Formula                                  | Practical Significance $d_{min}$ | Notation |\n",
    "|---------------------------|------------------------------------------|----------------------------------|----------|\n",
    "| Number of user-ids        | # user-ids that enroll in the free trial | 50                               |$enrolled$|\n",
    "\n",
    "- **Number of user-ids**: That is, number of users who enroll in the free trial. (dmin=50)\n",
    "    - This could possibly have been an invariant metric but since it is only recorded after the experimental change we're unable to use it as such. It might be affected by the variation.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Evaluation metrics are chosen to investigate the impact of the changes. They are usually tied to business goals (at least indirectly).\n",
    "\n",
    "| Metric Name      | Formula                           | Practical Significance $d_{min}$ | Notation             |\n",
    "|------------------|-----------------------------------|----------------------------------|----------------------|\n",
    "| Gross Conversion | $\\frac{enrolled}{cookies_{uniq}}$ | 1%                               | $Conversion_{gross}$ |\n",
    "| Retention        | $\\frac{payment}{enrolled}$        | 1%                               | $Retention$      |\n",
    "| Net Conversion   | $\\frac{payment}{cookies_{uniq}}$  | 0.75%                            | $Conversion_{net}$   |\n",
    "\n",
    "\n",
    "- **Gross conversion**: That is, number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the \"Start free trial\" button. (dmin= 0.01)\n",
    "    - This metric is going to measure whether we're successfully deterring users from enrolling in the free trial. We're expecting the gross conversion to go down ($H_1$).\n",
    "- **Retention**: That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by number of user-ids to complete checkout. (dmin=0.01)\n",
    "    - This metric is going to measure the probability that we're successfully retaining users through the free trial. We're expecting retention to increase ($H_1$).\n",
    "- **Net conversion**: That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the \"Start free trial\" button. (dmin= 0.0075)\n",
    "    - This metric is going to measure whether we remain able to funnel users through the free trial that are likely to succeed in completing a course. We're hoping that this metric will stay unchanged ($H_1$).\n",
    "\n",
    "NOTE: _Any place \"unique cookies\" are mentioned, the uniqueness is determined by day. (That is, the same cookie visiting on different days would be counted twice.) User-ids are automatically unique since the site does not allow the same user-id to enroll twice._\n",
    "\n",
    "NOTE: $d_{min}$ _is the difference necessary to be practically significant._\n",
    "\n",
    "**Essential Measures for Launch Recommendation:** It is necessary to see a significant drop by at least 1% in Gross Conversion, without significantly (1% and 0.75% respectively) affecting Retention and Net Conversion. \n",
    "\n",
    "## Variability\n",
    "\n",
    "Variability and variability estimates are essential to making reliable estimates on experiment sizing. Certain types of metrics allow us to make variance estimates analytically (e.g. binomial,  normal, difference between two counts and rates) others are more difficult as they rely on the underlying distribution.\n",
    "\n",
    "A summary of common estimates for metrics:\n",
    "\n",
    "| type of metric | distribution | requirement | est. Variance |\n",
    "|---|---|---|\n",
    "| probability | binomial (normal) | probability value | $\\frac{\\hat{p}(1-\\hat{p})}{N}$ |\n",
    "| mean | normal | std. estimate | $\\frac{\\hat{\\sigma}^2}{N}$ |\n",
    "| median / percentile | varies | - | depends on underlying distribution |\n",
    "| difference (count) | normal (often) | variance est. of the compared counts | $var(x)+var(y)$ |\n",
    "| rates | poisson (often) | mean | $\\bar{x}$ (mean) |\n",
    "| ratios | varies | - | depends on underlying distribution of numerator and denominator |\n",
    "\n",
    "The following metrics are baseline metrics and I assume are some avg. metric measured over a longer timeframe.\n",
    "\n",
    "#### Baseline Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline = pd.read_csv('data/baseline.csv', header=None)\n",
    "baseline.columns = ['description','N=40000']\n",
    "\n",
    "# add notation for easier retrieval\n",
    "baseline.index = ['cookies_uniq','clicks_uniq','enrolled','ctp','conversion_gross','retention','conversion_net']\n",
    "\n",
    "# add practical significance levels (d_min)\n",
    "\n",
    "baseline['dmin'] = [3000,240,50,0.01,0.01,0.01,0.0075]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should be making ourselves familiar with the metrics we're working with. We'll categorize into types and unit of analysis. \n",
    "\n",
    "The type is important for our variance estimate and the unit of analysis will allow us to make a decision on whether this variance estimate is likely to be reliable. That is, if `unit of analysis != unit of diversion`, then our analytical esimate might not be reliable and we should consider an empirical estimate instead. \n",
    "\n",
    "_Remember the unit of diversion is 'cookies'._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline['type'] = ['count', 'count', 'count', 'probability', 'probability', 'probability', 'probability']\n",
    "baseline['unit'] = ['cookies', 'cookies', 'user-id', 'cookies','cookies','user-id','cookies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>type</th>\n",
       "      <th>unit</th>\n",
       "      <th>dmin</th>\n",
       "      <th>N=40000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cookies_uniq</th>\n",
       "      <td>Unique cookies to view course overview page pe...</td>\n",
       "      <td>count</td>\n",
       "      <td>cookies</td>\n",
       "      <td>3000.0000</td>\n",
       "      <td>40000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clicks_uniq</th>\n",
       "      <td>Unique cookies to click \"Start free trial\" per...</td>\n",
       "      <td>count</td>\n",
       "      <td>cookies</td>\n",
       "      <td>240.0000</td>\n",
       "      <td>3200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enrolled</th>\n",
       "      <td>Enrollments per day:</td>\n",
       "      <td>count</td>\n",
       "      <td>user-id</td>\n",
       "      <td>50.0000</td>\n",
       "      <td>660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctp</th>\n",
       "      <td>Click-through-probability on \"Start free trial\":</td>\n",
       "      <td>probability</td>\n",
       "      <td>cookies</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversion_gross</th>\n",
       "      <td>Probability of enrolling, given click:</td>\n",
       "      <td>probability</td>\n",
       "      <td>cookies</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.206250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retention</th>\n",
       "      <td>Probability of payment, given enroll:</td>\n",
       "      <td>probability</td>\n",
       "      <td>user-id</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversion_net</th>\n",
       "      <td>Probability of payment, given click</td>\n",
       "      <td>probability</td>\n",
       "      <td>cookies</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.109313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        description  \\\n",
       "cookies_uniq      Unique cookies to view course overview page pe...   \n",
       "clicks_uniq       Unique cookies to click \"Start free trial\" per...   \n",
       "enrolled                                       Enrollments per day:   \n",
       "ctp                Click-through-probability on \"Start free trial\":   \n",
       "conversion_gross             Probability of enrolling, given click:   \n",
       "retention                     Probability of payment, given enroll:   \n",
       "conversion_net                  Probability of payment, given click   \n",
       "\n",
       "                         type     unit       dmin       N=40000  \n",
       "cookies_uniq            count  cookies  3000.0000  40000.000000  \n",
       "clicks_uniq             count  cookies   240.0000   3200.000000  \n",
       "enrolled                count  user-id    50.0000    660.000000  \n",
       "ctp               probability  cookies     0.0100      0.080000  \n",
       "conversion_gross  probability  cookies     0.0100      0.206250  \n",
       "retention         probability  user-id     0.0100      0.530000  \n",
       "conversion_net    probability  cookies     0.0075      0.109313  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put columns in order\n",
    "baseline = baseline.loc[:,['description','type','unit','dmin','N=40000']]\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the type count excludes cookies_uniq, clicks_uniq and enrolled from being able to make any reliable variance estimate for these metrics. That is okay since they are not part of our evaluation metrics and we don't necessarily need estimates for them.\n",
    "\n",
    "For the other metrics we're working with probabilities and luckily we have an estimate available \n",
    "\n",
    "$$\\frac{\\hat{p}(1-\\hat{p})}{N}$$ \n",
    "\n",
    "Lastly, we'll need to check whether our `unit of diversion = unit of analysis`. This applies to all probability metrics except for retention (which uses `user-id` as a unit of analysis). Ideally, we would want to have an empirical estimate for this metric but we'll move on disregarding this disconnect for now.\n",
    "\n",
    "For this estimate we'll be using $N=5000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N=5000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cookies_uniq</th>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clicks_uniq</th>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enrolled</th>\n",
       "      <td>82.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctp</th>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversion_gross</th>\n",
       "      <td>0.206250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retention</th>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversion_net</th>\n",
       "      <td>0.109313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       N=5000\n",
       "cookies_uniq      5000.000000\n",
       "clicks_uniq        400.000000\n",
       "enrolled            82.500000\n",
       "ctp                  0.080000\n",
       "conversion_gross     0.206250\n",
       "retention            0.530000\n",
       "conversion_net       0.109313"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce counts to 5000 cookies (i.e. divide  each count by 8)\n",
    "baseline.loc[['cookies_uniq','clicks_uniq','enrolled'],'N=5000'] = baseline['N=40000'] / 8\n",
    "\n",
    "# Probabilities don't change by changing the counts proportionally\n",
    "baseline.loc[['ctp','conversion_gross','retention','conversion_net'],'N=5000'] = baseline.loc[['ctp','conversion_gross','retention','conversion_net'],'N=40000']\n",
    "\n",
    "baseline.loc[:,['N=5000']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These adjusted sample sizes are now going to be used to make our estimates for the Standard Deviations (i.e. the square root of the variance) of the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Standard Deviation Est.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ctp</th>\n",
       "      <td>0.003837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversion_gross</th>\n",
       "      <td>0.020231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retention</th>\n",
       "      <td>0.054949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversion_net</th>\n",
       "      <td>0.015602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Standard Deviation Est.\n",
       "ctp                              0.003837\n",
       "conversion_gross                 0.020231\n",
       "retention                        0.054949\n",
       "conversion_net                   0.015602"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate standard deviation for CTP with N = 5000\n",
    "p_ctp = baseline.loc['ctp','N=5000']\n",
    "N_ctp = baseline.loc['cookies_uniq','N=5000']\n",
    "baseline.loc['ctp','Standard Deviation Est.'] = np.sqrt(p_ctp * (1-p_ctp) / N_ctp)\n",
    "\n",
    "# Calculate standard deviation for conversion_gross with N = 5000 (respectively 400)\n",
    "p_conversion_gross = baseline.loc['conversion_gross','N=5000']\n",
    "N_conversion_gross = baseline.loc['clicks_uniq','N=5000']\n",
    "baseline.loc['conversion_gross','Standard Deviation Est.'] = np.sqrt(p_conversion_gross * (1-p_conversion_gross) / N_conversion_gross)\n",
    "\n",
    "# Calculate standard deviation for retention with N = 5000 (respectively 82.5)\n",
    "p_retention = baseline.loc['retention','N=5000']\n",
    "N_retention = baseline.loc['enrolled','N=5000']\n",
    "baseline.loc['retention','Standard Deviation Est.'] = np.sqrt(p_retention * (1-p_retention) / N_retention)\n",
    "\n",
    "# Calculate standard deviation for conversion_net with N = 5000 (respectively 400)\n",
    "p_conversion_net = baseline.loc['conversion_net','N=5000']\n",
    "N_conversion_net = baseline.loc['clicks_uniq','N=5000']\n",
    "baseline.loc['conversion_net','Standard Deviation Est.'] = np.sqrt(p_conversion_net * (1-p_conversion_net) / N_conversion_net)\n",
    "\n",
    "baseline.loc[['ctp','conversion_gross','retention','conversion_net'],['Standard Deviation Est.']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the variability of your metrics is essential to understand the sizing for your experiment and fine-tuning what metrics can and should be used for a given experiment (a high variability might not make much sense for experimenting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing \n",
    "\n",
    "The variability estimates above are not only essential to understanding whether a metric is usable (i.e. reliable) but also can be used for an experiment sizing. \n",
    "\n",
    "Let's consider our requirements. $\\alpha$ is our level of accepting a Type I Error and $\\beta$ our level of acceptance for a Type II Error. These are in an inverse relationship. Another restriction is our sample size $N$. We're therefore facing an optimization problem between these three factors. Given that we usually have some best practice numbers for $\\alpha$ and $\\beta$ we usually make $N$ the variable to optimize.\n",
    "\n",
    "To better understand the sizing formula let's first consider what generally $d$ looks like\n",
    "\n",
    "$$d = |p_{control} - p_{exp}|$$\n",
    "\n",
    "Simple enough, the difference between two values is the absolute value of one subtracted by the other. Since we're estimating we're now going to consider replacing the values above with educated guesses and we get:\n",
    "\n",
    "$$d = |p_{baseline} - p_{dmin}|$$\n",
    "\n",
    "We're using the baseline value as our control p and the baseline value + practical significance level $p_{dmin} = p_{baseline} + d_{min}$ as the smallest change that is valuable to detect.\n",
    "\n",
    "_Note: Minimizing for d is our practical significance level._\n",
    "\n",
    "Let's now consider the smallest possible values that we start rejecting each of our requirements for:\n",
    "\n",
    "$$for\\ \\alpha_{min},\\ \\ \\ z_{1-\\alpha}*\\frac{\\sqrt{p_{baseline}(1-p_{baseline})}}{\\sqrt{N}}$$\n",
    "\n",
    "$$for\\ \\beta_{min},\\ \\ \\ -z_{1-\\beta}*\\frac{\\sqrt{p_{dmin}(1-p_{dmin})}}{\\sqrt{N}}$$\n",
    "\n",
    "_Note: We're taking the negative of the result for $\\beta_{min}$ as we're interested in the lower tail of the distribution (where we have the largest overlap between the two distributions) in an experiment setup where we assume the evaluation metrics increase_\n",
    "\n",
    "$\\sqrt{p(1-p)}$ is simply the standard deviation (based on the variance estimate of a probability), so we'll replace it with $SD$ for better readability. Now, using the formula with the minimized $\\alpha$ and $\\beta$ we get,\n",
    "\n",
    "$$d_{min} = z_{1-\\alpha}*\\frac{SD_{baseline}}{N} - (-z_{1-\\beta}*\\frac{SD_{dmin}}{N})$$\n",
    "\n",
    "Translating this formula to $N$ we get the final workable formula,\n",
    "\n",
    "$$N=\\bigg(\\frac{Z_{1-\\alpha}SD_1+Z_{1âˆ’\\beta}SD_2}{d_{min}}\\bigg)^2$$\n",
    "\n",
    "Deriving from the variance estimates table seen at the beginning of this notebook we can derive $SD_1$ and $SD_2$. However, note that we have to use pooled variances for this calculation as we're looking at two samples and this looks as follows\n",
    "\n",
    "$$SD_1=\\sqrt{2*p_{baseline}(1-p_{baseline})}\\ \\ \\ \\ \\ SD_2=\\sqrt{p_{baseline}(1-p_{baseline}) + (p_{dmin})(1-(p_{dmin}))}$$\n",
    "\n",
    "$SD_1$ computes the estimated standard deviation of the baseline parameter and $SD_2$ the estimated standard deviation of the baseline parameter + the practical significance level.\n",
    "\n",
    "The following is taken from this [blog post](http://www.alfredo.motta.name/ab-testing-from-scratch/) translation of the sizing script written in R. This does require some prior knowledge of power and significance to follow. \n",
    "\n",
    "Basically, we'll need a few things:\n",
    "\n",
    "| Input | Notation |\n",
    "|---|---|\n",
    "| Statistical Significance level (Probability of Type I Error / alpha) | $\\alpha$ |\n",
    "| Practical Significance level (closest True Parameter of interest) | $d_{min}$ |\n",
    "| Standard Error Estimate | $s$ |\n",
    "| Beta (Probability of Type II Error) | $\\beta$ |\n",
    "\n",
    "The significance level and beta can simply be defined using experience and best practices (often $\\alpha=0.05$ and $\\beta=0.2$). The practical signficance level can be chosen using business goals and experience. Lastly, since we usually should work with a baseline in A/B Testing we can often use that Standard Error Estimate as an approximation for the Standard Error Estimate of the True Parameter (the parameter we're trying to detect via the experiment).\n",
    "\n",
    "Now that we have the inputs we'll want to get the smallest possible sample size (N) that fulfills the $<=\\beta$ requirement. In our calculator we'll be doing that using trial and error, we're going to compute $\\beta$ for a bunch of Ns and pick the smallest N at which beta crosses the desired value.\n",
    "\n",
    "_Note: This sample size calculator assumes a two-tailed test and a normal distribution and is not usable for any other underlying distributions or test types. It also makes a simplification for z\\* which is usually fine since we're largely just making an estimate but usually using one of the [online calculators](http://www.evanmiller.org/ab-testing/sample-size.html) is advisable._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from math import sqrt\n",
    "\n",
    "def get_SDs(p_base,d_min):\n",
    "    sd_base = sqrt(2*p_base*(1-p_base))\n",
    "    p_dmin = p_base+d_min\n",
    "    sd_dmin = sqrt(p_base*(1-p_base)+p_dmin*(1-p_dmin))\n",
    "    return [sd_base,sd_dmin]\n",
    "\n",
    "# Inputs:\n",
    "# The desired alpha for a two-tailed test.\n",
    "# Returns: The z-critical value\n",
    "# Note: -norm.ppf(alpha / 2) equals norm.ppf(1 - (alpha / 2)) due to the symmetric shape of the normal distribution\n",
    "def get_z_star(alpha):\n",
    "    return(norm.ppf(alpha))\n",
    "    \n",
    "# Inputs:\n",
    "#   s: The standard error of the metric with N=1 in each group\n",
    "#   d_min: The practical significance level\n",
    "#   Ns: The sample sizes to try\n",
    "#   alpha: The desired alpha level of the test\n",
    "#   beta: The desired beta level of the test\n",
    "# Returns: The smallest N out of the given Ns that will achieve the desired\n",
    "#          beta. There should be at least N samples in each group of the experiment.\n",
    "#          If none of the given Ns will work, returns -1. N is the number of\n",
    "#          samples in each group.\n",
    "def required_size(SDs, d_min, alpha=0.05, beta=0.2):\n",
    "    z_1_minus_a = get_z_star(1-alpha/2)\n",
    "    z_1_minus_b = get_z_star(1-beta)\n",
    "    \n",
    "    return ((z_1_minus_a * SDs[0] + z_1_minus_b * SDs[1]) / d_min) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Size\n",
    "\n",
    "Since there are three evaluation metrics, we'll need to size the experiment for each of them and choose the largest one that we choose to include. But first the inputs need to be defined.\n",
    "\n",
    "_Define Inputs_\n",
    "\n",
    "Statistical Significance levels vary depending on how many metrics you're including in your analysis. The statistical error actually adds up pretty quickly, to counteract this we can lower alpha for each individual metric to maintain a desired $\\alpha_{overall}$. Often we're using the Bonferroni Correction to do this:\n",
    "\n",
    "$$\\alpha_{individual} = \\frac{\\alpha_{overall}}{m} = \\frac{0.05}{3} = 0.0167$$\n",
    "\n",
    "Our new alpha with three evaluation metrics is thus \n",
    "\n",
    "$\\alpha = 0.0167$\n",
    "\n",
    "$\\beta = 0.2$\n",
    "\n",
    "The practical significance and standard error estimate depends on the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.0167\n",
    "beta = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Gross Conversion_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_baseline: 0.20625\n",
      "d_min: 0.01\n",
      "N: 34419\n"
     ]
    }
   ],
   "source": [
    "p_gross = baseline.loc['conversion_gross','N=5000']\n",
    "dmin_gross = baseline.loc['conversion_gross','dmin']\n",
    "print('p_baseline: {}\\nd_min: {}'.format(p_gross,dmin_gross))\n",
    "\n",
    "N_gross = int(round(required_size(get_SDs(p_gross,dmin_gross), dmin_gross, alpha=alpha, beta=beta)))\n",
    "\n",
    "print('N: {}'.format(N_gross))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need 34419 cookies per group. Since our baseline conversion from pageview (cookies_uniq) to click is 0.08, we'll divide the number of cookies by that conversion rate. Lastly, we'll need to double the amount to get to the right amount of pageview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necessary number of pageviews (cookies_uniq) in total: 860475.0\n"
     ]
    }
   ],
   "source": [
    "ctp = baseline.loc['ctp','N=5000']\n",
    "cookies_gross = round(N_gross/ctp * 2)\n",
    "\n",
    "print(\"Necessary number of pageviews (cookies_uniq) in total: {}\".format(cookies_gross))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Retention_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_baseline: 0.53\n",
      "d_min: 0.01\n",
      "N: 52114\n"
     ]
    }
   ],
   "source": [
    "p_retention = baseline.loc['retention','N=5000']\n",
    "dmin_retention = baseline.loc['retention','dmin']\n",
    "print('p_baseline: {}\\nd_min: {}'.format(p_retention,dmin_retention))\n",
    "\n",
    "N_retention = int(round(required_size(get_SDs(p_retention,dmin_retention), dmin_retention, alpha=alpha, beta=beta)))\n",
    "\n",
    "print('N: {}'.format(N_retention))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need 52114 user-ids to get a relevant sample. In this case, we're actually two conversion steps apart from pageviews. So to get to the necessary amount of pageviews \n",
    "(cookies_uniq) we'll need to expand the term by the enrollment probability (0.20625) as well as by the click probability (0.08)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necessary number of pageviews (cookies_uniq) in total: 6316848.0\n"
     ]
    }
   ],
   "source": [
    "cookies_retention = round(N_retention/p_gross/ctp * 2)\n",
    "\n",
    "print(\"Necessary number of pageviews (cookies_uniq) in total: {}\".format(cookies_retention))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Net Conversion_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_baseline: 0.1093125\n",
      "d_min: 0.0075\n",
      "N: 36505\n"
     ]
    }
   ],
   "source": [
    "p_net = baseline.loc['conversion_net','N=5000']\n",
    "dmin_net = baseline.loc['conversion_net','dmin']\n",
    "print('p_baseline: {}\\nd_min: {}'.format(p_net,dmin_net))\n",
    "\n",
    "N_net = int(round(required_size(get_SDs(p_net,dmin_net), dmin_net, alpha=alpha, beta=beta)))\n",
    "\n",
    "print('N: {}'.format(N_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necessary number of pageviews (cookies_uniq) in total: 912625.0\n"
     ]
    }
   ],
   "source": [
    "cookies_net = round(N_net/ctp * 2)\n",
    "\n",
    "print(\"Necessary number of pageviews (cookies_uniq) in total: {}\".format(cookies_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration and Exposure\n",
    "\n",
    "We can see in the previous cells that Retention requires a much higher sample size compared to the other two metrics. In this step we'll evaluate the feasibility of acquiring a sample large enough by looking at the duration necessary to acquire such a sample. \n",
    "\n",
    "It is important that an experiment is performed over at least the amount of time to capture the effect. This is tricky and depends on the experiment at hand but usually we want to at least have a duration of approximately two weeks (two cycles). We also don't want the experiment to last too long as we might capture a completely new population or other things change in the environment that make our groups not comparable.\n",
    "\n",
    "One lever to extend or tighten duration is Exposure. We usually only want to show our experiment to a subset of users to minimize the risk and ensure a consistent experience for most of our users. For our experiment we'll set the exposure to 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration (Gross Conversion): 36 days\n",
      "duration (Retention): 263 days\n",
      "duration (Net Conversion): 38 days\n"
     ]
    }
   ],
   "source": [
    "daily_cookies = baseline.loc['cookies_uniq','N=40000']\n",
    "exposure = 0.6\n",
    "\n",
    "duration_gross = cookies_gross/ (daily_cookies * exposure)\n",
    "duration_retention = cookies_retention/ (daily_cookies * exposure)\n",
    "duration_net = cookies_net/ (daily_cookies * exposure)\n",
    "\n",
    "print(\"\"\"duration (Gross Conversion): {} days\n",
    "duration (Retention): {} days\n",
    "duration (Net Conversion): {} days\"\"\".format(int(round(duration_gross)),int(round(duration_retention)),int(round(duration_net))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our suspicion has been confirmed. Retention will cause the experiment to be more than 7 times longer than would be necessary for the other two metrics. Thus, we'll drop _Retention_ from our roster of evaluation metrics. This means $\\alpha_{overall}$ needs to be recalculated and the sample size as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration (Gross Conversion): 33 days\n",
      "duration (Net Conversion): 35 days\n",
      "0.025\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05 / 2\n",
    "\n",
    "N_gross = int(round(required_size(get_SDs(p_gross,dmin_gross), dmin_gross, alpha=alpha, beta=beta)))\n",
    "N_net = int(round(required_size(get_SDs(p_net,dmin_net), dmin_net, alpha=alpha, beta=beta)))\n",
    "\n",
    "cookies_gross = N_gross / ctp * 2\n",
    "cookies_net = N_net / ctp * 2\n",
    "\n",
    "duration_gross = cookies_gross/ (daily_cookies * exposure)\n",
    "duration_net = cookies_net/ (daily_cookies * exposure)\n",
    "\n",
    "print(\"\"\"duration (Gross Conversion): {} days\n",
    "duration (Net Conversion): {} days\"\"\".format(int(round(duration_gross)),int(round(duration_net))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The results are in. Due to a business decision the experiment ran for 37 days but that only leaves us with 23 days of valid results (payment tracking comes in with a 14 day delay). \n",
    "\n",
    "_Note: We're going to ignore the validity issues of the length of the experiment for the purpose of this analysis and assume $\\alpha = 0.025$ is applicable here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cookies_uniq</th>\n",
       "      <th>clicks_uniq</th>\n",
       "      <th>enrolled</th>\n",
       "      <th>payments</th>\n",
       "      <th>ctp</th>\n",
       "      <th>conversion_gross</th>\n",
       "      <th>conversion_net</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sat, Oct 11</td>\n",
       "      <td>7723</td>\n",
       "      <td>687</td>\n",
       "      <td>134.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.088955</td>\n",
       "      <td>0.195051</td>\n",
       "      <td>0.101892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sun, Oct 12</td>\n",
       "      <td>9102</td>\n",
       "      <td>779</td>\n",
       "      <td>147.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.085586</td>\n",
       "      <td>0.188703</td>\n",
       "      <td>0.089859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon, Oct 13</td>\n",
       "      <td>10511</td>\n",
       "      <td>909</td>\n",
       "      <td>167.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.086481</td>\n",
       "      <td>0.183718</td>\n",
       "      <td>0.104510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue, Oct 14</td>\n",
       "      <td>9871</td>\n",
       "      <td>836</td>\n",
       "      <td>156.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.084693</td>\n",
       "      <td>0.186603</td>\n",
       "      <td>0.125598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed, Oct 15</td>\n",
       "      <td>10014</td>\n",
       "      <td>837</td>\n",
       "      <td>163.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.083583</td>\n",
       "      <td>0.194743</td>\n",
       "      <td>0.076464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thu, Oct 16</td>\n",
       "      <td>9670</td>\n",
       "      <td>823</td>\n",
       "      <td>138.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.085109</td>\n",
       "      <td>0.167679</td>\n",
       "      <td>0.099635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fri, Oct 17</td>\n",
       "      <td>9008</td>\n",
       "      <td>748</td>\n",
       "      <td>146.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.083037</td>\n",
       "      <td>0.195187</td>\n",
       "      <td>0.101604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sat, Oct 18</td>\n",
       "      <td>7434</td>\n",
       "      <td>632</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.085015</td>\n",
       "      <td>0.174051</td>\n",
       "      <td>0.110759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sun, Oct 19</td>\n",
       "      <td>8459</td>\n",
       "      <td>691</td>\n",
       "      <td>131.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.081688</td>\n",
       "      <td>0.189580</td>\n",
       "      <td>0.086831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mon, Oct 20</td>\n",
       "      <td>10667</td>\n",
       "      <td>861</td>\n",
       "      <td>165.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.080716</td>\n",
       "      <td>0.191638</td>\n",
       "      <td>0.112660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Tue, Oct 21</td>\n",
       "      <td>10660</td>\n",
       "      <td>867</td>\n",
       "      <td>196.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.081332</td>\n",
       "      <td>0.226067</td>\n",
       "      <td>0.121107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wed, Oct 22</td>\n",
       "      <td>9947</td>\n",
       "      <td>838</td>\n",
       "      <td>162.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.084247</td>\n",
       "      <td>0.193317</td>\n",
       "      <td>0.109785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Thu, Oct 23</td>\n",
       "      <td>8324</td>\n",
       "      <td>665</td>\n",
       "      <td>127.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.079889</td>\n",
       "      <td>0.190977</td>\n",
       "      <td>0.084211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fri, Oct 24</td>\n",
       "      <td>9434</td>\n",
       "      <td>673</td>\n",
       "      <td>220.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.071338</td>\n",
       "      <td>0.326895</td>\n",
       "      <td>0.181278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sat, Oct 25</td>\n",
       "      <td>8687</td>\n",
       "      <td>691</td>\n",
       "      <td>176.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.079544</td>\n",
       "      <td>0.254703</td>\n",
       "      <td>0.185239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sun, Oct 26</td>\n",
       "      <td>8896</td>\n",
       "      <td>708</td>\n",
       "      <td>161.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.079586</td>\n",
       "      <td>0.227401</td>\n",
       "      <td>0.146893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Mon, Oct 27</td>\n",
       "      <td>9535</td>\n",
       "      <td>759</td>\n",
       "      <td>233.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.079601</td>\n",
       "      <td>0.306983</td>\n",
       "      <td>0.163373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Tue, Oct 28</td>\n",
       "      <td>9363</td>\n",
       "      <td>736</td>\n",
       "      <td>154.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.078607</td>\n",
       "      <td>0.209239</td>\n",
       "      <td>0.123641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Wed, Oct 29</td>\n",
       "      <td>9327</td>\n",
       "      <td>739</td>\n",
       "      <td>196.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.079232</td>\n",
       "      <td>0.265223</td>\n",
       "      <td>0.116373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Thu, Oct 30</td>\n",
       "      <td>9345</td>\n",
       "      <td>734</td>\n",
       "      <td>167.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.078545</td>\n",
       "      <td>0.227520</td>\n",
       "      <td>0.102180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Fri, Oct 31</td>\n",
       "      <td>8890</td>\n",
       "      <td>706</td>\n",
       "      <td>174.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.079415</td>\n",
       "      <td>0.246459</td>\n",
       "      <td>0.143059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sat, Nov 1</td>\n",
       "      <td>8460</td>\n",
       "      <td>681</td>\n",
       "      <td>156.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.080496</td>\n",
       "      <td>0.229075</td>\n",
       "      <td>0.136564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sun, Nov 2</td>\n",
       "      <td>8836</td>\n",
       "      <td>693</td>\n",
       "      <td>206.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.078429</td>\n",
       "      <td>0.297258</td>\n",
       "      <td>0.096681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mon, Nov 3</td>\n",
       "      <td>9437</td>\n",
       "      <td>788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Tue, Nov 4</td>\n",
       "      <td>9420</td>\n",
       "      <td>781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.082909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Wed, Nov 5</td>\n",
       "      <td>9570</td>\n",
       "      <td>805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.084117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Thu, Nov 6</td>\n",
       "      <td>9921</td>\n",
       "      <td>830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Fri, Nov 7</td>\n",
       "      <td>9424</td>\n",
       "      <td>781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.082874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sat, Nov 8</td>\n",
       "      <td>9010</td>\n",
       "      <td>756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sun, Nov 9</td>\n",
       "      <td>9656</td>\n",
       "      <td>825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.085439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Mon, Nov 10</td>\n",
       "      <td>10419</td>\n",
       "      <td>874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Tue, Nov 11</td>\n",
       "      <td>9880</td>\n",
       "      <td>830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.084008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Wed, Nov 12</td>\n",
       "      <td>10134</td>\n",
       "      <td>801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.079041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Thu, Nov 13</td>\n",
       "      <td>9717</td>\n",
       "      <td>814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Fri, Nov 14</td>\n",
       "      <td>9192</td>\n",
       "      <td>735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.079961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Sat, Nov 15</td>\n",
       "      <td>8630</td>\n",
       "      <td>743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.086095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Sun, Nov 16</td>\n",
       "      <td>8970</td>\n",
       "      <td>722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.080491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  cookies_uniq  clicks_uniq  enrolled  payments       ctp  \\\n",
       "0   Sat, Oct 11          7723          687     134.0      70.0  0.088955   \n",
       "1   Sun, Oct 12          9102          779     147.0      70.0  0.085586   \n",
       "2   Mon, Oct 13         10511          909     167.0      95.0  0.086481   \n",
       "3   Tue, Oct 14          9871          836     156.0     105.0  0.084693   \n",
       "4   Wed, Oct 15         10014          837     163.0      64.0  0.083583   \n",
       "5   Thu, Oct 16          9670          823     138.0      82.0  0.085109   \n",
       "6   Fri, Oct 17          9008          748     146.0      76.0  0.083037   \n",
       "7   Sat, Oct 18          7434          632     110.0      70.0  0.085015   \n",
       "8   Sun, Oct 19          8459          691     131.0      60.0  0.081688   \n",
       "9   Mon, Oct 20         10667          861     165.0      97.0  0.080716   \n",
       "10  Tue, Oct 21         10660          867     196.0     105.0  0.081332   \n",
       "11  Wed, Oct 22          9947          838     162.0      92.0  0.084247   \n",
       "12  Thu, Oct 23          8324          665     127.0      56.0  0.079889   \n",
       "13  Fri, Oct 24          9434          673     220.0     122.0  0.071338   \n",
       "14  Sat, Oct 25          8687          691     176.0     128.0  0.079544   \n",
       "15  Sun, Oct 26          8896          708     161.0     104.0  0.079586   \n",
       "16  Mon, Oct 27          9535          759     233.0     124.0  0.079601   \n",
       "17  Tue, Oct 28          9363          736     154.0      91.0  0.078607   \n",
       "18  Wed, Oct 29          9327          739     196.0      86.0  0.079232   \n",
       "19  Thu, Oct 30          9345          734     167.0      75.0  0.078545   \n",
       "20  Fri, Oct 31          8890          706     174.0     101.0  0.079415   \n",
       "21   Sat, Nov 1          8460          681     156.0      93.0  0.080496   \n",
       "22   Sun, Nov 2          8836          693     206.0      67.0  0.078429   \n",
       "23   Mon, Nov 3          9437          788       NaN       NaN  0.083501   \n",
       "24   Tue, Nov 4          9420          781       NaN       NaN  0.082909   \n",
       "25   Wed, Nov 5          9570          805       NaN       NaN  0.084117   \n",
       "26   Thu, Nov 6          9921          830       NaN       NaN  0.083661   \n",
       "27   Fri, Nov 7          9424          781       NaN       NaN  0.082874   \n",
       "28   Sat, Nov 8          9010          756       NaN       NaN  0.083907   \n",
       "29   Sun, Nov 9          9656          825       NaN       NaN  0.085439   \n",
       "30  Mon, Nov 10         10419          874       NaN       NaN  0.083885   \n",
       "31  Tue, Nov 11          9880          830       NaN       NaN  0.084008   \n",
       "32  Wed, Nov 12         10134          801       NaN       NaN  0.079041   \n",
       "33  Thu, Nov 13          9717          814       NaN       NaN  0.083771   \n",
       "34  Fri, Nov 14          9192          735       NaN       NaN  0.079961   \n",
       "35  Sat, Nov 15          8630          743       NaN       NaN  0.086095   \n",
       "36  Sun, Nov 16          8970          722       NaN       NaN  0.080491   \n",
       "\n",
       "    conversion_gross  conversion_net  \n",
       "0           0.195051        0.101892  \n",
       "1           0.188703        0.089859  \n",
       "2           0.183718        0.104510  \n",
       "3           0.186603        0.125598  \n",
       "4           0.194743        0.076464  \n",
       "5           0.167679        0.099635  \n",
       "6           0.195187        0.101604  \n",
       "7           0.174051        0.110759  \n",
       "8           0.189580        0.086831  \n",
       "9           0.191638        0.112660  \n",
       "10          0.226067        0.121107  \n",
       "11          0.193317        0.109785  \n",
       "12          0.190977        0.084211  \n",
       "13          0.326895        0.181278  \n",
       "14          0.254703        0.185239  \n",
       "15          0.227401        0.146893  \n",
       "16          0.306983        0.163373  \n",
       "17          0.209239        0.123641  \n",
       "18          0.265223        0.116373  \n",
       "19          0.227520        0.102180  \n",
       "20          0.246459        0.143059  \n",
       "21          0.229075        0.136564  \n",
       "22          0.297258        0.096681  \n",
       "23               NaN             NaN  \n",
       "24               NaN             NaN  \n",
       "25               NaN             NaN  \n",
       "26               NaN             NaN  \n",
       "27               NaN             NaN  \n",
       "28               NaN             NaN  \n",
       "29               NaN             NaN  \n",
       "30               NaN             NaN  \n",
       "31               NaN             NaN  \n",
       "32               NaN             NaN  \n",
       "33               NaN             NaN  \n",
       "34               NaN             NaN  \n",
       "35               NaN             NaN  \n",
       "36               NaN             NaN  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in experiment and control data\n",
    "exp = pd.read_csv('data/experiment.csv',parse_dates=['Date'])\n",
    "cont = pd.read_csv('data/control.csv',parse_dates=['Date'])\n",
    "\n",
    "# Rename columns to above naming convention\n",
    "header = ['date','cookies_uniq','clicks_uniq','enrolled','payments']\n",
    "exp.columns = header\n",
    "cont.columns = header\n",
    "\n",
    "# Compute metrics\n",
    "# Note that retention is not necessary to compute as we've dropped that evaluation metric earlier\n",
    "exp['ctp'] = exp['clicks_uniq'] / exp['cookies_uniq']\n",
    "exp['conversion_gross'] = exp['enrolled'] / exp['clicks_uniq']\n",
    "exp['conversion_net'] = exp['payments'] / exp['clicks_uniq']\n",
    "\n",
    "cont['ctp'] = cont['clicks_uniq'] / cont['cookies_uniq']\n",
    "cont['conversion_gross'] = cont['enrolled'] / cont['clicks_uniq']\n",
    "cont['conversion_net'] = cont['payments'] / cont['clicks_uniq']\n",
    "\n",
    "cont.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks\n",
    "\n",
    "These initial test ensure that our experiment properly executed and reduces the chance that we've had any blunders in the test set up.\n",
    "\n",
    "These tests are done on the predefined invariant metrics:\n",
    "\n",
    "- Number of cookies         _| # cookies on course overview page_\n",
    "- Number of clicks          _| # cookies clicked on button_ \n",
    "- Click-Through-Probability\n",
    "\n",
    "#### Number of cookies\n",
    "\n",
    "The number of cookies or pageviews were the Unit of Diversion and have therefore been randomized. This means we can look at this unit in a way as a coin flip with p=0.5. The Standard Error should therefore be\n",
    "\n",
    "$$SE = \\sqrt{\\frac{p(1-p)}{N}},\\ with\\ p = 0.5$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Confidence Interval (CI) is 0.4984941609 and 0.5015058391.\n",
      "The Experiment Group split: 0.49905436515\n",
      "Pass: True\n"
     ]
    }
   ],
   "source": [
    "# Total Number of cookies in experimental und control group\n",
    "exp_cookies_total = float(exp.loc[:22,'cookies_uniq'].sum())\n",
    "cont_cookies_total = float(cont.loc[:22,'cookies_uniq'].sum())\n",
    "\n",
    "cookies_total = exp_cookies_total + cont_cookies_total\n",
    "\n",
    "# exp fraction of total cookies (should not be significantly different from 0.5)\n",
    "exp_split = exp_cookies_total / cookies_total\n",
    "\n",
    "# Confidence interval of a binomial distribution of p = 0.5\n",
    "p_equal = 0.5\n",
    "se = sqrt(p_equal*(1-p_equal) / cookies_total)\n",
    "z = -norm.ppf(alpha)\n",
    "m = z*se\n",
    "ci = (p_equal - m, p_equal + m)\n",
    "print('The Confidence Interval (CI) is {} and {}.\\nThe Experiment Group split: {}\\nPass: {}'.format(ci[0],ci[1],exp_split,exp_split > ci[0] and exp_split < ci[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to check the split of one group here as one depends on the other (all assigned to one or the other by definition). In our case the split was successful and is not significantly different from what we'd expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Clicks\n",
    "\n",
    "The next check will explore the invariant metric of clicks. We'll be going a similar route but use a pooled variance instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
